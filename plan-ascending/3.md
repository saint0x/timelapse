# Phase 3: Checkpoint Journal & Incremental Updates

**Status**: 30% complete - Core types and journal implemented

## Checkpoint Data Structures

- [x] Create `crates/journal/Cargo.toml`
  - [x] Dependencies: sled, serde, bincode, ulid, chrono, parking_lot
  - [x] Add core as workspace dependency

- [x] Implement `checkpoint.rs`: Core types
  ```rust
  pub struct Checkpoint {
      pub id: CheckpointId,
      pub parent: Option<CheckpointId>,
      pub root_tree: Blake3Hash,
      pub touched_paths: SmallVec<[PathBuf; 8]>,
      pub meta: CheckpointMeta,
  }

  pub type CheckpointId = ulid::Ulid;  // Sortable by timestamp

  pub struct CheckpointMeta {
      pub timestamp: i64,           // Unix timestamp (microseconds)
      pub reason: CheckpointReason,
      pub size_delta: i64,          // Bytes added/removed
      pub stats: CheckpointStats,
  }

  pub enum CheckpointReason {
      FsBatch,       // Automatic from watcher
      Manual,        // User-triggered
      Restore,       // After restore operation
      Publish,       // Before JJ publish
      GcCompact,     // GC compaction
  }

  pub struct CheckpointStats {
      pub files_changed: u32,
      pub bytes_added: u64,
      pub bytes_removed: u64,
  }
  ```

- [x] Implement checkpoint ID derivation
  - [x] Use ULID: sortable, time-ordered, 128-bit
  - [x] `Ulid::new()` generates ID using current timestamp
  - [x] IDs are monotonically increasing (sorted by time)

- [x] Implement serialization
  - [x] Use bincode for compact binary encoding
  - [x] `Checkpoint::serialize(&self) -> Result<Vec<u8>>`
  - [x] `Checkpoint::deserialize(bytes: &[u8]) -> Result<Checkpoint>`
  - [x] Added Serialize/Deserialize derives to all types

## Journal Implementation

- [x] Implement `journal.rs`: Append-only log
  ```rust
  pub struct Journal {
      db: sled::Db,
      index: RwLock<BTreeMap<CheckpointId, u64>>,  // id -> seq
      seq_counter: AtomicU64,
  }
  ```

- [x] Initialize journal database
  - [x] `open(path: &Path) -> Result<Journal>`
  - [x] Open sled DB at `.tl/journal/checkpoints.db`
  - [x] Build in-memory index on startup (BTreeMap)
  - [x] Rebuilds index from all checkpoints on startup

- [x] Implement append operation
  - [x] `append(&self, checkpoint: &Checkpoint) -> Result<u64>`
    - [x] Assign monotonic sequence number via AtomicU64
    - [x] Serialize checkpoint with bincode
    - [x] Write to sled tree: `seq -> checkpoint_bytes`
    - [x] Update in-memory index: `checkpoint.id -> seq`
    - [x] Fsync for durability
    - [x] Return sequence number

- [x] Implement lookup operations
  - [x] `get(&self, id: &Ulid) -> Result<Option<Checkpoint>>`
    - [x] Lookup seq in index (O(1))
    - [x] Read from sled DB
    - [x] Deserialize
  - [x] `latest(&self) -> Result<Option<Checkpoint>>`
    - [x] Get max sequence from index
    - [x] Return most recent checkpoint

- [x] Implement range queries
  - [x] `since(&self, timestamp_ms: u64) -> Result<Vec<Checkpoint>>`
    - [x] Uses ULID timestamp component for filtering
  - [x] `last_n(&self, count: usize) -> Result<Vec<Checkpoint>>`
    - [x] Efficient: use BTreeMap index for filtering
  - [x] `all_checkpoint_ids(&self) -> Result<HashSet<Ulid>>`
  - [x] `count(&self) -> usize`
  - [x] `delete(&self, id: &Ulid) -> Result<()>`

- [ ] Implement parent chain traversal
  - [ ] `ancestors(&self, id: CheckpointId) -> Result<Vec<Checkpoint>>`
    - [ ] Follow parent pointers backward
    - [ ] Return in reverse chronological order
  - [ ] `common_ancestor(&self, a: CheckpointId, b: CheckpointId) -> Result<Option<CheckpointId>>`
    - [ ] Find merge base (for diffs)
  - [ ] **Note**: Deferred to when needed for diff/log commands

## PathMap: State Cache

- [ ] Implement `pathmap.rs`: In-memory working state
  ```rust
  pub struct PathMap {
      entries: FxHashMap<PathBuf, Entry>,  // path -> entry
      root_tree_hash: Blake3Hash,
  }

  // Entry from core::tree
  ```

- [ ] Implement persistence
  - [ ] `save(&self, path: &Path) -> Result<()>`
    - [ ] Serialize to `.tl/state/pathmap.bin`
    - [ ] Binary format (sorted entries for compactness):
      ```
      magic: "SNP1"
      root_tree: [u8; 32]
      entry_count: u32
      entries: [(path, entry), ...] sorted by path
      ```
    - [ ] Atomic write (tmp + rename)

  - [ ] `load(path: &Path) -> Result<PathMap>`
    - [ ] Read from disk
    - [ ] Deserialize
    - [ ] Verify root_tree matches current HEAD

- [ ] Implement verification & repair
  - [ ] `verify(&self, expected_tree: Blake3Hash) -> bool`
    - [ ] Compare root_tree_hash with expected
  - [ ] `rebuild_from_journal(&mut self, journal: &Journal) -> Result<()>`
    - [ ] If verification fails: rebuild from journal
    - [ ] Replay checkpoints from last known good state
    - [ ] Update entries incrementally

## Incremental Tree Update Algorithm

- [ ] Implement `incremental.rs`: Core algorithm
  ```rust
  pub struct IncrementalUpdater {
      store: Arc<Store>,
      current_map: PathMap,
  }
  ```

- [ ] Implement **Step A**: Coalesce and normalize dirty paths
  - [ ] `normalize_dirty_paths(&self, dirty: Vec<PathBuf>) -> Result<Vec<PathBuf>>`
    - [ ] Convert to repo-relative paths
    - [ ] Filter: drop `.tl/`, `.git/`
    - [ ] Deduplicate paths
    - [ ] Expand directory paths (shallow scan for file children)
    - [ ] Return candidate paths

- [ ] Implement **Step B**: Reconcile each candidate path
  - [ ] `reconcile_path(&mut self, path: &Path) -> Result<Option<Entry>>`
    - [ ] **Case 1: File exists**
      ```rust
      let metadata = fs::metadata(path)?;
      if metadata.is_file() {
          // Quick fingerprint check
          let fp = (metadata.len(), metadata.modified()?);
          if cached_fp_matches(fp) {
              return Ok(cached_entry);  // Fast path: skip hash
          }
          // Slow path: hash file
          let hash = hash_file(path)?;
          if cached_hash_matches(hash) {
              return Ok(update_mode_only(hash, metadata.mode()));
          }
          // Store new blob
          store_blob_if_missing(hash, path)?;
          return Ok(Entry { kind: File, mode, blob_hash: hash });
      }
      if metadata.is_symlink() {
          let target = fs::read_link(path)?;
          let hash = hash_bytes(target.as_bytes());
          return Ok(Entry { kind: Symlink, mode: 0o120000, blob_hash: hash });
      }
      ```
    - [ ] **Case 2: Path does not exist**
      ```rust
      return Ok(None);  // Deletion
      ```
    - [ ] Update current_map with new/removed entry

- [ ] Implement **Step C**: Produce new root tree hash
  - [ ] **Option 1 (MVP)**: Flat tree serialization
    ```rust
    fn update_tree_flat(&mut self) -> Result<Blake3Hash> {
        // Serialize all entries (sorted)
        let tree = Tree::from_map(&self.current_map.entries);
        let tree_bytes = tree.serialize();
        let tree_hash = hash_bytes(&tree_bytes);

        // Store tree object
        self.store.write_tree(&tree)?;

        Ok(tree_hash)
    }
    ```
    - [ ] O(N) where N = total tracked files
    - [ ] Acceptable for MVP (with debouncing, not every keystroke)

  - [ ] **Option 2 (Future)**: Merkle directory tree
    - [ ] **Do NOT implement for MVP** (optimization for later)
    - [ ] Document approach for future:
      ```
      - Build directory hierarchy
      - Hash each directory node (contains sorted children)
      - Update only directory path to root
      - O(changed_files * log(depth))
      ```

- [ ] Implement **Step D**: Append checkpoint to journal
  - [ ] `create_checkpoint(&self, touched_paths: Vec<PathBuf>) -> Result<Checkpoint>`
    ```rust
    let checkpoint = Checkpoint {
        id: Ulid::new(),
        parent: current_head_id,
        root_tree: new_tree_hash,
        touched_paths: touched_paths.into(),  // SmallVec
        meta: CheckpointMeta {
            timestamp: now_micros(),
            reason: FsBatch,
            size_delta: compute_delta(),
            stats: compute_stats(),
        },
    };
    journal.append(checkpoint)?;
    update_head(checkpoint.id)?;
    ```

- [ ] Implement pathmap snapshot policy
  - [ ] `should_snapshot(seq: u64) -> bool`
    - [ ] Snapshot every 100 checkpoints
    - [ ] Or every 10 minutes (whichever comes first)
  - [ ] Lazy snapshot in background (don't block checkpoint creation)

## Retention & GC

- [ ] Implement `retention.rs`: Retention policies
  ```rust
  pub struct RetentionPolicy {
      pub dense_count: usize,       // Keep last N checkpoints (default: 2000)
      pub dense_window: Duration,   // Keep last T time (default: 24h)
      pub pins: Vec<Pin>,            // Never GC pinned checkpoints
  }

  pub struct Pin {
      pub checkpoint_id: CheckpointId,
      pub name: String,
      pub created: i64,
  }
  ```

- [ ] Implement pin management
  - [ ] `pin(&self, id: CheckpointId, name: String) -> Result<()>`
    - [ ] Write to `.tl/refs/pins/<name>`
    - [ ] File contains checkpoint ID
  - [ ] `unpin(&self, name: &str) -> Result<()>`
    - [ ] Remove `.tl/refs/pins/<name>`
  - [ ] `list_pins(&self) -> Result<Vec<Pin>>`
    - [ ] Read all files in `.tl/refs/pins/`

- [ ] Implement GC algorithm: Mark & Sweep
  - [ ] `gc(&self, journal: &Journal, store: &Store) -> Result<GcStats>`
    ```rust
    // Step 1: Determine live checkpoint set
    let mut live_checkpoints = HashSet::new();

    // Add pinned checkpoints
    for pin in list_pins()? {
        live_checkpoints.insert(pin.checkpoint_id);
    }

    // Add last N checkpoints
    for cp in journal.last_n(dense_count)? {
        live_checkpoints.insert(cp.id);
    }

    // Add checkpoints in dense_window
    let cutoff = now() - dense_window;
    for cp in journal.since(cutoff)? {
        live_checkpoints.insert(cp.id);
    }

    // Step 2: Walk reachable trees and blobs
    let mut live_trees = HashSet::new();
    let mut live_blobs = HashSet::new();

    for cp_id in &live_checkpoints {
        let cp = journal.get_by_id(*cp_id)?.unwrap();
        live_trees.insert(cp.root_tree);

        // Load tree and mark all blobs
        let tree = store.read_tree(cp.root_tree)?;
        for entry in tree.entries.values() {
            live_blobs.insert(entry.blob_hash);
        }
    }

    // Step 3: Delete unreferenced objects
    delete_unreferenced_checkpoints(&live_checkpoints)?;
    delete_unreferenced_trees(&live_trees)?;
    delete_unreferenced_blobs(&live_blobs)?;
    ```

- [ ] Implement background GC
  - [ ] Acquire `.tl/locks/gc.lock` (prevent concurrent GC)
  - [ ] Run GC in background task (never block checkpoint creation)
  - [ ] Log GC metrics: checkpoints deleted, bytes freed

- [ ] Implement GC safety
  - [ ] Never delete objects referenced by live checkpoints
  - [ ] Verify lock before deletion
  - [ ] Atomic deletion (mark deleted, then remove)

## Correctness Guarantees

- [ ] Implement double-stat verification
  - [ ] `verify_stable_read(path: &Path) -> Result<Vec<u8>>`
    ```rust
    let stat1 = fs::metadata(path)?;
    let data = fs::read(path)?;
    let stat2 = fs::metadata(path)?;

    if stat1.modified()? != stat2.modified()? {
        // File changed during read - requeue
        return Err("file modified during read");
    }

    Ok(data)
    ```

- [ ] Implement crash recovery
  - [ ] On startup: verify journal consistency
  - [ ] Check if pathmap.bin matches HEAD checkpoint
  - [ ] If mismatch: rebuild from journal
  - [ ] Delete incomplete checkpoint writes in `.tl/tmp/`

## Testing

- [ ] Unit tests for checkpoint serialization
  - [ ] Test bincode round-trip
  - [ ] Test compact size (< 300 bytes per checkpoint)
  - [ ] Test ULID ordering (time-based)

- [ ] Unit tests for journal
  - [ ] Test append + lookup
  - [ ] Test range queries
  - [ ] Test parent chain traversal

- [ ] Unit tests for incremental update
  - [ ] Test reconcile_path (all cases: exists, deleted, symlink)
  - [ ] Test tree update (flat serialization)
  - [ ] Test pathmap persistence

- [ ] Unit tests for GC
  - [ ] Test mark & sweep (verify only unreferenced objects deleted)
  - [ ] Test pin protection (pinned checkpoints never deleted)
  - [ ] Test retention policies (dense_count, dense_window)

- [ ] Integration tests
  - [ ] Create checkpoint → verify tree hash
  - [ ] Create 100 checkpoints → GC → verify retention
  - [ ] Simulate crash → restart → verify recovery

## Performance Benchmarks

- [ ] Benchmark checkpoint creation
  - [ ] Small change (1 file): < 10ms target
  - [ ] Medium change (10 files): < 50ms target
  - [ ] Large change (100 files): < 500ms target

- [ ] Benchmark incremental update
  - [ ] Measure hash skipping (mtime fast path)
  - [ ] Measure tree serialization overhead
  - [ ] Profile hotspots

- [ ] Benchmark GC
  - [ ] GC on 10k checkpoints: < 5 seconds
  - [ ] Measure blob deduplication effectiveness

## Memory Optimization

- [ ] Verify SmallVec usage
  - [ ] `touched_paths` uses stack allocation for < 8 paths
  - [ ] Measure heap allocation reduction

- [ ] Verify lazy loading
  - [ ] Tree objects loaded on-demand (not all in memory)
  - [ ] Pathmap eviction (only recent checkpoints cached)

- [ ] Profile memory under load
  - [ ] Target: < 30MB for journal + pathmap
  - [ ] 10k checkpoints in journal: < 3MB (300 bytes each)
