# Phase 1: Foundation & Core Storage

## Workspace Setup

- [ ] Update root `Cargo.toml` with workspace definition
  - [ ] Define workspace members: `seer-core`, `seer-watcher`, `seer-journal`, `seer-jj`, `seer-cli`
  - [ ] Define `[workspace.dependencies]` for version consistency
  - [ ] Add blake3, memmap2, sled, tokio, parking_lot, crossbeam-channel, dashmap
  - [ ] Add smallvec, ahash, bytes, serde, bincode
  - [ ] Add notify, clap, tracing, jj-lib, chrono, ulid, thiserror, anyhow

- [ ] Create crate directory structure
  - [ ] `mkdir -p crates/seer-core/src`
  - [ ] `mkdir -p crates/seer-watcher/src`
  - [ ] `mkdir -p crates/seer-journal/src`
  - [ ] `mkdir -p crates/seer-jj/src`
  - [ ] `mkdir -p crates/seer-cli/src`
  - [ ] `mkdir -p benches`

## seer-core: Hash Module (hash.rs)

- [ ] Create `crates/seer-core/Cargo.toml`
  - [ ] Add workspace dependency references
  - [ ] Core deps: blake3, memmap2, parking_lot, dashmap, smallvec, ahash, bytes, serde, bincode

- [ ] Implement `hash.rs`: BLAKE3 wrapper
  - [ ] `Blake3Hash` type: `struct Blake3Hash([u8; 32])`
  - [ ] Implement `Copy`, `Clone`, `Hash`, `Eq`, `PartialEq`, `Debug`
  - [ ] `hash_bytes(data: &[u8]) -> Blake3Hash` - basic hashing
  - [ ] `hash_file(path: &Path) -> Result<Blake3Hash>` - streaming hash for files
  - [ ] `hash_file_mmap(path: &Path) -> Result<Blake3Hash>` - memory-mapped for large files (>4MB)
  - [ ] `IncrementalHasher` wrapper around `blake3::Hasher`
  - [ ] Hex encoding/decoding: `to_hex()`, `from_hex()`
  - [ ] Parallel hashing support using rayon for multiple files

## seer-core: Blob Module (blob.rs)

- [ ] Define blob header format
  ```rust
  struct BlobHeaderV1 {
      magic: [u8; 4],     // "SNB1"
      flags: u8,          // compression, type
      orig_len: u64,      // original size
      stored_len: u64,    // compressed size (if compressed)
  }
  ```
  - [ ] Flag bits: bit0=compressed (zstd), bit1-7=reserved
  - [ ] Implement header serialization/deserialization

- [ ] Implement `Blob` type
  - [ ] `struct Blob { hash: Blake3Hash, size: u64, compressed: bool }`
  - [ ] `from_bytes(data: &[u8]) -> Result<(Blob, Vec<u8>)>` - create blob from bytes
  - [ ] `to_bytes(&self, data: &[u8]) -> Result<Vec<u8>>` - serialize with header
  - [ ] Compression: zstd level 3 for blobs > 4KB
  - [ ] Decompression on read

- [ ] Implement `BlobStore`
  - [ ] `DashMap<Blake3Hash, Arc<Blob>>` for in-memory cache
  - [ ] `BufferPool<BytesMut>` - pre-allocated 64KB buffers (object pool pattern)
  - [ ] `write_blob(hash: Blake3Hash, data: &[u8]) -> Result<()>`
    - [ ] Atomic write: write to `.snap/tmp/ingest/<uuid>` then rename to `objects/blobs/<hh>/<rest>`
    - [ ] First 2 hex chars as subdirectory (reduce inode pressure)
  - [ ] `read_blob(hash: Blake3Hash) -> Result<Vec<u8>>`
    - [ ] Check cache first
    - [ ] Memory-map for large blobs (zero-copy)
    - [ ] Decompress if needed
  - [ ] `has_blob(hash: Blake3Hash) -> bool` - check existence without reading
  - [ ] LRU eviction policy (configurable max cache size, default 50MB)

- [ ] Memory optimization
  - [ ] Pre-allocate buffer pool: 16 buffers of 64KB each
  - [ ] Path interning: use `Arc<Path>` for blob paths
  - [ ] Lazy loading: only load metadata on cache miss

## seer-core: Tree Module (tree.rs)

- [ ] Define tree entry types
  ```rust
  enum EntryKind {
      File,
      Symlink,
      Submodule,  // optional for MVP
  }

  struct Entry {
      kind: EntryKind,
      mode: u32,           // Unix permission bits
      blob_hash: Blake3Hash,
  }
  ```

- [ ] Define tree structure (flat for MVP)
  ```rust
  struct Tree {
      entries: FxHashMap<SmallVec<[u8; 64]>, Entry>,  // path -> entry
  }
  ```
  - [ ] Use `FxHashMap` (faster for small keys)
  - [ ] `SmallVec` for paths (stack allocation for paths < 64 bytes)

- [ ] Implement tree serialization (TreeV1 format)
  ```
  TreeV1:
    magic: [u8; 4] = "SNT1"
    entry_count: u32
    repeated entry:
      path_len: u16
      path_bytes: [u8; path_len]
      kind: u8              // 0=file, 1=symlink, 2=submodule
      mode: u32
      blob_hash: [u8; 32]
  ```
  - [ ] Entries must be sorted lexicographically by path (deterministic hash)
  - [ ] `serialize() -> Vec<u8>` - encode to bytes
  - [ ] `deserialize(bytes: &[u8]) -> Result<Tree>` - decode from bytes

- [ ] Implement tree hashing
  - [ ] `hash(&self) -> Blake3Hash` - hash serialized tree bytes
  - [ ] Tree hash is deterministic (same content = same hash)

- [ ] Implement tree diffing
  - [ ] `diff(old: &Tree, new: &Tree) -> TreeDiff`
  - [ ] `TreeDiff` contains: added, removed, modified entries
  - [ ] Efficient: only compare changed subtrees (use hash equality)

- [ ] Implement incremental tree update
  - [ ] `update_entries(base: &Tree, changes: Vec<(Path, Option<Entry>)>) -> Tree`
  - [ ] Apply changes to base tree (insert/remove entries)
  - [ ] Return new tree without full rescan

## seer-core: Store Module (store.rs)

- [ ] Define `.snap/` directory layout
  ```
  .snap/
    config.toml
    HEAD
    locks/
      daemon.lock
      gc.lock
    journal/
      ops.log
      ops.log.idx
    objects/
      blobs/
      trees/
    refs/
      pins/
      heads/
    state/
      pathmap.bin
      watcher.state
      metrics.json
    tmp/
      ingest/
      gc/
  ```

- [ ] Implement `Store` initialization
  - [ ] `init(repo_root: &Path) -> Result<Store>` - create `.snap/` structure
  - [ ] Create all subdirectories
  - [ ] Initialize `config.toml` with defaults
  - [ ] Initialize empty `ops.log`

- [ ] Implement `Store` struct
  - [ ] `struct Store { root: PathBuf, blob_store: BlobStore, tree_cache: DashMap<Blake3Hash, Arc<Tree>> }`
  - [ ] `open(repo_root: &Path) -> Result<Store>` - open existing store
  - [ ] Validate `.snap/` structure on open

- [ ] Implement tree storage
  - [ ] `write_tree(tree: &Tree) -> Result<Blake3Hash>`
    - [ ] Serialize tree
    - [ ] Compute hash
    - [ ] Write to `objects/trees/<hh>/<rest>`
    - [ ] Atomic write (tmp + rename)
  - [ ] `read_tree(hash: Blake3Hash) -> Result<Tree>`
    - [ ] Check cache first
    - [ ] Read from disk + deserialize
    - [ ] Cache result

- [ ] Implement atomic write helpers
  - [ ] `atomic_write(tmp_dir: &Path, target: &Path, data: &[u8]) -> Result<()>`
    - [ ] Write to temp file in tmp_dir
    - [ ] Fsync temp file
    - [ ] Rename to target
    - [ ] Fsync parent directory (crash safety)

- [ ] Path normalization utilities
  - [ ] `normalize_path(path: &Path) -> Result<PathBuf>`
    - [ ] Convert to relative path with `/` separator
    - [ ] Reject `..` and absolute paths
    - [ ] Remove `./` prefix

- [ ] Ignore `.snap/` and `.git/` helpers
  - [ ] `should_ignore(path: &Path) -> bool`
  - [ ] Never watch or checkpoint `.snap/` or `.git/`

## Testing

- [ ] Unit tests for hash.rs
  - [ ] Test hash consistency (same input = same hash)
  - [ ] Test hex encoding/decoding round-trip
  - [ ] Test incremental hasher

- [ ] Unit tests for blob.rs
  - [ ] Test blob serialization/deserialization
  - [ ] Test compression/decompression
  - [ ] Test buffer pool reuse
  - [ ] Test atomic writes

- [ ] Unit tests for tree.rs
  - [ ] Test deterministic serialization (sorted entries)
  - [ ] Test tree hashing (same tree = same hash)
  - [ ] Test tree diffing
  - [ ] Test incremental update

- [ ] Unit tests for store.rs
  - [ ] Test store initialization
  - [ ] Test atomic write operations
  - [ ] Test path normalization
  - [ ] Test ignore rules

## Memory Profiling Baseline

- [ ] Create benchmark for blob operations
  - [ ] Measure memory usage for 1000 blob writes
  - [ ] Measure buffer pool efficiency
  - [ ] Target: < 10MB for idle state

- [ ] Create benchmark for tree operations
  - [ ] Measure memory for 10k-entry tree
  - [ ] Measure incremental update performance
  - [ ] Target: < 5ms for small updates (1-5 files)
